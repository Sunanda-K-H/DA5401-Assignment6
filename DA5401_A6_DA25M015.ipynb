{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac549c02",
   "metadata": {},
   "source": [
    "# DA5401 A6: Imputation via Regression for Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632fa7a3",
   "metadata": {},
   "source": [
    "**Given Objective:**\n",
    "\n",
    "This assignment challenges you to apply linear and non-linear regression to impute missing values in a dataset. The effectiveness of your imputation methods will be measured indirectly by assessing the performance of a subsequent classification task, comparing the regression-based approach against simpler imputation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b98787",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "You are a machine learning engineer working on a credit risk assessment project. You have been provided with the UCI Credit Card Default Clients Dataset. This dataset has missing values in several important feature columns. The presence of missing data prevents the immediate application of many classification algorithms.\n",
    "\n",
    "Your task is to implement three different strategies for handling the missing data and then use the resulting clean datasets to train and evaluate a classification model. This will demonstrate how the choice of imputation technique significantly impacts final model performance.\n",
    "\n",
    "You will submit a Jupyter Notebook with your complete code, visualizations, and a plausible story that explains your findings. The notebook should be well-commented, reproducible, and easy to follow.\n",
    "\n",
    "\n",
    "### Dataset \n",
    "\n",
    "- **UCI Credit Card Default Clients Dataset (with missing values)**: Kaggle - [Credit Card Default Clients Dataset](https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset)\n",
    "\t- *Note: While the original UCI dataset is relatively clean, for this assignment, you should artificially introduce ***Missing At Random (MAR)*** values (e.g., replace 5% of the values in the 'AGE' and 'BILL_AMT' columns with NaN) before starting Part A, to simulate a real-world scenario with a substantial missing data problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b486f6",
   "metadata": {},
   "source": [
    "**About the Dataset**\n",
    "\n",
    "\n",
    "This dataset contains information on *default payments*, *demographic factors*, *credit data*, *payment history*, and *bill statements* of credit card clients in Taiwan from **April 2005 to September 2005**.\n",
    "\n",
    "<u>Feature Descriptions:</u>\n",
    "\n",
    "| **Feature Name** | **Description** |\n",
    "|-------------------|-----------------|\n",
    "| ID | ID of each client |\n",
    "| LIMIT_BAL | Amount of given credit in NT dollars (includes individual and family/supplementary credit) |\n",
    "| SEX | Gender (1 = male, 2 = female) |\n",
    "| EDUCATION | Education level (1 = graduate school, 2 = university, 3 = high school, 4 = others, 5 = unknown, 6 = unknown) |\n",
    "| MARRIAGE | Marital status (1 = married, 2 = single, 3 = others) |\n",
    "| AGE | Age in years |\n",
    "| PAY_0 | Repayment status in September 2005 (-1 = pay duly, 1 = payment delay for one month, 2 = delay for two months, …, 8 = delay for eight months, 9 = delay for nine months and above) |\n",
    "| PAY_2 | Repayment status in August 2005 (same scale as above) |\n",
    "| PAY_3 | Repayment status in July 2005 (same scale as above) |\n",
    "| PAY_4 | Repayment status in June 2005 (same scale as above) |\n",
    "| PAY_5 | Repayment status in May 2005 (same scale as above) |\n",
    "| PAY_6 | Repayment status in April 2005 (same scale as above) |\n",
    "| BILL_AMT1 | Amount of bill statement in September 2005 (NT dollars) |\n",
    "| BILL_AMT2 | Amount of bill statement in August 2005 (NT dollars) |\n",
    "| BILL_AMT3 | Amount of bill statement in July 2005 (NT dollars) |\n",
    "| BILL_AMT4 | Amount of bill statement in June 2005 (NT dollars) |\n",
    "| BILL_AMT5 | Amount of bill statement in May 2005 (NT dollars) |\n",
    "| BILL_AMT6 | Amount of bill statement in April 2005 (NT dollars) |\n",
    "| PAY_AMT1 | Amount of previous payment in September 2005 (NT dollars) |\n",
    "| PAY_AMT2 | Amount of previous payment in August 2005 (NT dollars) |\n",
    "| PAY_AMT3 | Amount of previous payment in July 2005 (NT dollars) |\n",
    "| PAY_AMT4 | Amount of previous payment in June 2005 (NT dollars) |\n",
    "| PAY_AMT5 | Amount of previous payment in May 2005 (NT dollars) |\n",
    "| PAY_AMT6 | Amount of previous payment in April 2005 (NT dollars) |\n",
    "| default.payment.next.month | Default payment indicator (1 = yes, 0 = no) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38808609",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c450d",
   "metadata": {},
   "source": [
    "## Part A: Data Preprocessing and Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279d642",
   "metadata": {},
   "source": [
    "### Question 1 : Load and Prepare Data\n",
    "\n",
    "Load the dataset and, as instructed in the note above, **artificially introduce MAR missing values** (5-10% in 2-3 numerical feature columns).\n",
    "The target variable is 'default payment next month'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1be813a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8b8177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version of the dataset.\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "folder_path = kagglehub.dataset_download(\"uciml/default-of-credit-card-clients-dataset\")\n",
    "file_name = os.listdir(folder_path)[0]\n",
    "data_path = os.path.join(folder_path, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fee6cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Read the CSV file into a dataframe.\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Preview the data.\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07f9e5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sampels:  30000\n",
      "Number of features:  25\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sampels: \", data.shape[0])\n",
    "print(\"Number of features: \", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c0f9af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_dist = pd.DataFrame(data['default.payment.next.month'].value_counts())\n",
    "class_dist = class_dist.reset_index()\n",
    "class_map = {0:'no', 1:'yes'}\n",
    "class_dist['default.payment.next.month'] = class_dist['default.payment.next.month'].map(class_map)\n",
    "\n",
    "plt.bar(class_dist['default.payment.next.month'],class_dist['count'], color=['green', 'red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f8e4934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default.payment.next.month</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no</td>\n",
       "      <td>23364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yes</td>\n",
       "      <td>6636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default.payment.next.month  count\n",
       "0                         no  23364\n",
       "1                        yes   6636"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5f8b3",
   "metadata": {},
   "source": [
    "- The target variable `default.payment.next.month` is **imbalanced**.  \n",
    "- Around `23364 clients` did **not default** on their payments.  \n",
    "- Only about `6,600 clients` **defaulted** on their payments.  \n",
    "- This imbalance means that the model might become biased toward predicting no default.  \n",
    "- To address this, techniques like **stratified splitting**, **resampling**, or **class weighting** can be used during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0d66ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in SEX : [2 1]\n",
      "\n",
      "\n",
      "Unique values in EDUCATION : [2 1 3 5 4 6 0]\n",
      "\n",
      "\n",
      "Unique values in MARRIAGE : [1 2 3 0]\n",
      "\n",
      "\n",
      "Unique values in PAY_0 : [ 2 -1  0 -2  1  3  4  8  7  5  6]\n",
      "\n",
      "\n",
      "Unique values in PAY_2 : [ 2  0 -1 -2  3  5  7  4  1  6  8]\n",
      "\n",
      "\n",
      "Unique values in PAY_3 : [-1  0  2 -2  3  4  6  7  1  5  8]\n",
      "\n",
      "\n",
      "Unique values in PAY_4 : [-1  0 -2  2  3  4  5  7  6  1  8]\n",
      "\n",
      "\n",
      "Unique values in PAY_5 : [-2  0 -1  2  3  5  4  7  8  6]\n",
      "\n",
      "\n",
      "Unique values in PAY_6 : [-2  2  0 -1  3  6  4  7  8  5]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for unique values in the categorical columns.\n",
    "\n",
    "category_cols = ['SEX', 'EDUCATION', \"MARRIAGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"]\n",
    "\n",
    "for col in category_cols:\n",
    "    print(f\"Unique values in {col} : {data[col].unique()}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c68d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning.\n",
    "\n",
    "data_clean = data.copy()\n",
    "\n",
    "data_clean['MARRIAGE'] = data_clean['MARRIAGE'].replace({0: 3})\n",
    "data_clean['EDUCATION'] = data_clean['EDUCATION'].replace({0: 4, 5: 4, 6: 4})\n",
    "\n",
    "\n",
    "pay_cols = [c for c in data.columns if c.startswith(\"PAY_\")]\n",
    "\n",
    "for c in pay_cols:\n",
    "    data_clean[c] = data_clean[c].replace({0: -1, -2: -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91736aa9",
   "metadata": {},
   "source": [
    "- The dataset contained undocumented values in categorical columns such as EDUCATION, MARRIAGE, and PAY_*.\n",
    "- The value `0` in MARRIAGE was recoded as `3` to represent \"Other.\"\n",
    "- The values `0`, `5`, and `6` in EDUCATION were replaced with `4` to represent \"Other.\"\n",
    "- In PAY_* columns, values `0` and `-2` were replaced with `-1` to indicate \"no delay\" or \"payment made duly.\"\n",
    "- These replacements help align the data with the dataset documentation.\n",
    "- No rows were removed, only inconsistent categorical values were corrected.\n",
    "- This ensures a consistent and interpretable dataset for analysis and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a6d39ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of missing values in three randomly selected columns.\n",
    "\n",
    "missing_frac = {\n",
    "    'AGE': 0.05,         \n",
    "    'BILL_AMT2': 0.08,   \n",
    "    'PAY_AMT4': 0.07     \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71841965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce MAR into the data.\n",
    "\n",
    "for col, frac in missing_frac.items():\n",
    "    data_clean.loc[data_clean.sample(frac=frac, random_state=42).index, col] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea18ea95",
   "metadata": {},
   "source": [
    "### Question 2 : Imputation Strategy 1: Simple Imputation (Baseline)\n",
    "\n",
    "- Create a clean dataset copy (Dataset A).\n",
    "- For each column with missing values, fill the missing values with the median of that column. Explain why the median is often preferred over the mean for\n",
    "imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0afeddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the clean data to work with for imputation.\n",
    "\n",
    "datasetA = data_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "161c1205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['AGE', 'BILL_AMT2', 'PAY_AMT4'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the list of columns to be imputed.\n",
    "\n",
    "missing_cols = missing_frac.keys()\n",
    "missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5670eda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AGE             34.0\n",
       "BILL_AMT2    21407.5\n",
       "PAY_AMT4      1500.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the median value\n",
    "\n",
    "missing_med = datasetA[missing_cols].median()\n",
    "missing_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71485848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the missing values\n",
    "\n",
    "datasetA.loc[:, missing_cols] = datasetA[missing_cols].fillna(missing_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66fe709b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AGE          0\n",
       "BILL_AMT2    0\n",
       "PAY_AMT4     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the imputation\n",
    "\n",
    "datasetA[missing_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876cc24",
   "metadata": {},
   "source": [
    "### Question 3 : Imputation Strategy 2: Regression Imputation (Linear) \n",
    "\n",
    "- Create a second clean dataset copy (Dataset B).\n",
    "- For a single column (your choice) with missing values, use a Linear Regression model to predict the missing values based on all other non-missing features. Explain the underlying assumption of this method (Missing At Random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d428aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second copy of the clean data\n",
    "\n",
    "datasetB = data_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8235170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'AGE' # Define the column name that is currently being targeted for imputation.\n",
    "\n",
    "cols_na = [x for x in missing_cols if x != target] # Define columns that are not the target and have missing values.\n",
    "\n",
    "# Create training and testing sets.\n",
    "trainB_imputation = datasetB[datasetB[target].notna()].drop(columns=cols_na).copy() \n",
    "testB_imputation = datasetB[datasetB[target].isna()].drop(columns=cols_na).copy()\n",
    "\n",
    "# Separate features (X) and the target (y) for the imputation model.\n",
    "XtrainB_imputation = trainB_imputation.drop(columns=[target])\n",
    "ytrainB_imputation = trainB_imputation[target]\n",
    "XtestB_imputation = testB_imputation.drop(columns=[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97f51f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LinearRegression model from scikit-learn.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression() # Initialize the Linear Regression model.\n",
    "lr_model.fit(XtrainB_imputation,ytrainB_imputation)  # Train the model using the non-missing rows.\n",
    "y_predB_imputation = lr_model.predict(XtestB_imputation) # Use the trained model to predict the missing target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28469e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predB__imputation_rounded = np.round(y_predB_imputation) # Round the predicted values to the nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab3990ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetB.loc[datasetB[target].isna(), target] = y_predB__imputation_rounded  # Impute the missing values in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7716068a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetB[target].isna().sum() # Verify the imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc65b0bb",
   "metadata": {},
   "source": [
    "### Question 4 : Imputation Strategy 3: Regression Imputation (Non-Linear)\n",
    "\n",
    "- Create a third clean dataset copy (Dataset C).\n",
    "- For the same column as in Strategy 2, use a non-linear regression model (e.g., K-Nearest Neighbors Regression or Decision Tree Regression) to predict the\n",
    "missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e419466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetC = data_clean.copy() # Create a third copy of the clean data\n",
    "\n",
    "# Split data into training (non-missing target) and testing (missing target) sets\n",
    "trainC_imputation = datasetC[datasetC[target].notna()].drop(columns=cols_na).copy()\n",
    "testC = datasetC[datasetC[target].isna()].drop(columns=cols_na).copy()\n",
    "\n",
    "# Separate features (X) and the target (y) for the imputation model\n",
    "XtrainC_imputation = trainC_imputation.drop(columns=[target])\n",
    "ytrainC_imputation = trainC_imputation[target]\n",
    "XtestC_imputation = testC.drop(columns=[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9aa62cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Since kNN depends on features, standardization is performed on the data.\n",
    "scaler_imputation = StandardScaler() # Initialize the StandardScaler object.\n",
    "XtrainC_sc_imputation = scaler_imputation.fit_transform(XtrainC_imputation) # Fit the scaler to training features and transform them.\n",
    "XtestC_sc_imputation  = scaler_imputation.transform(XtestC_imputation) # Transform the test features using the fitted scaler.\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5, weights=\"distance\") # Initialize the model.\n",
    "knn.fit(XtrainC_sc_imputation, ytrainC_imputation)  # Train the model using the non-missing rows.\n",
    "y_predC_imputation = knn.predict(XtestC_sc_imputation) # Use the trained model to predict the missing target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df3e7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip predictions to the actual range of the target\n",
    "y_predC_imputation = np.round(np.clip(y_predC_imputation, ytrainC_imputation.min(), ytrainC_imputation.max())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b5931f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetC.loc[datasetC[target].isna(), target] = y_predC_imputation # Impute the missing values in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8b3b57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetC[target].isna().sum() # Verify the imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a365f1a2",
   "metadata": {},
   "source": [
    "## Part B: Model Training and Performance Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc81fe",
   "metadata": {},
   "source": [
    "### Question 1 : Data Split\n",
    "\n",
    "For each of the three imputed datasets (A, B, C), split the data into training and testing sets. Also, create a fourth dataset (Dataset D) by simply **removing all rows** that contain any missing values (Listwise Deletion). Split Dataset D into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21a24166",
   "metadata": {},
   "outputs": [],
   "source": [
    "XdatasetA = datasetA.iloc[:,:-1]\n",
    "XdatasetB = datasetB.iloc[:,:-1]\n",
    "XdatasetC = datasetC.iloc[:,:-1]\n",
    "\n",
    "XdatasetA.drop(columns=cols_na, inplace=True)\n",
    "XdatasetB.drop(columns=cols_na, inplace=True)\n",
    "XdatasetC.drop(columns=cols_na, inplace=True)\n",
    "\n",
    "ydatasetA = datasetA.iloc[:,-1]\n",
    "ydatasetB = datasetB.iloc[:,-1]\n",
    "ydatasetC = datasetC.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e4d74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainA, X_testA, y_trainA, y_testA = train_test_split(XdatasetA,ydatasetA,random_state=1,test_size=0.20,shuffle=True, stratify=ydatasetA)\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(XdatasetB,ydatasetB,random_state=1,test_size=0.20,shuffle=True, stratify=ydatasetB)\n",
    "X_trainC, X_testC, y_trainC, y_testC = train_test_split(XdatasetC,ydatasetC,random_state=1,test_size=0.20,shuffle=True, stratify=ydatasetC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fdfdec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetD = data_clean.copy()\n",
    "datasetD.dropna(axis = 0, inplace=True)\n",
    "\n",
    "XdatasetD = datasetD.iloc[:,:-1]\n",
    "XdatasetD.drop(columns=cols_na, inplace=True)\n",
    "\n",
    "ydatasetD = datasetD.iloc[:,-1]\n",
    "\n",
    "X_trainD, X_testD, y_trainD, y_testD = train_test_split(XdatasetD,ydatasetD,random_state=1,test_size=0.20,shuffle=True, stratify=ydatasetD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df18d551",
   "metadata": {},
   "source": [
    "### Question 2 : Classifier Setup\n",
    "\n",
    "Standardize the features in all four datasets (A, B, C, D) using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d11508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerA = StandardScaler()\n",
    "XtrainA_scaled = scalerA.fit_transform(X_trainA)\n",
    "XtestA_scaled  = scalerA.transform(X_testA)\n",
    "\n",
    "\n",
    "scalerB = StandardScaler()\n",
    "XtrainB_scaled = scalerB.fit_transform(X_trainB)\n",
    "XtestB_scaled  = scalerB.transform(X_testB)\n",
    "\n",
    "scalerC = StandardScaler()\n",
    "XtrainC_scaled = scalerC.fit_transform(X_trainC)\n",
    "XtestC_scaled  = scalerC.transform(X_testC)\n",
    "\n",
    "scalerD = StandardScaler()\n",
    "XtrainD_scaled = scalerD.fit_transform(X_trainD)\n",
    "XtestD_scaled  = scalerD.transform(X_testD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77abc49b",
   "metadata": {},
   "source": [
    "### Question 3 : Model Evaluation\n",
    "\n",
    "Train a Logistic Regression classifier on the training set of each of the four datasets (A, B, C, D). Evaluate the performance of each model on its\n",
    "respective test set using a full Classification Report (Accuracy, Precision, Recall, F1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e2fbb1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Model A: Median Imputation: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.956     0.891      4673\n",
      "           1      0.679     0.326     0.440      1327\n",
      "\n",
      "    accuracy                          0.817      6000\n",
      "   macro avg      0.756     0.641     0.665      6000\n",
      "weighted avg      0.799     0.817     0.791      6000\n",
      "\n",
      "\n",
      "\n",
      " Model B: Linear Regression Imputation: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.956     0.891      4673\n",
      "           1      0.679     0.326     0.440      1327\n",
      "\n",
      "    accuracy                          0.817      6000\n",
      "   macro avg      0.756     0.641     0.665      6000\n",
      "weighted avg      0.799     0.817     0.791      6000\n",
      "\n",
      "\n",
      "\n",
      " Model C: Non-Linear Regression Imputation: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.956     0.891      4673\n",
      "           1      0.679     0.326     0.440      1327\n",
      "\n",
      "    accuracy                          0.817      6000\n",
      "   macro avg      0.756     0.641     0.665      6000\n",
      "weighted avg      0.799     0.817     0.791      6000\n",
      "\n",
      "\n",
      "\n",
      " Model D: Listwise Deletion: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.834     0.959     0.892      4296\n",
      "           1      0.698     0.328     0.447      1224\n",
      "\n",
      "    accuracy                          0.820      5520\n",
      "   macro avg      0.766     0.644     0.669      5520\n",
      "weighted avg      0.804     0.820     0.793      5520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "\n",
    "# Model A\n",
    "clfa = LogisticRegression(max_iter=500)\n",
    "clfa.fit(XtrainA_scaled, y_trainA)\n",
    "yhatA = clfa.predict(XtestA_scaled)\n",
    "print(\"\\n\\n Model A: Median Imputation: \")\n",
    "print(classification_report(y_testA, yhatA, digits=3))\n",
    "accA = accuracy_score(y_testA, yhatA)\n",
    "precA, recA, f1A, _ = precision_recall_fscore_support(y_testA, yhatA, average=\"binary\", zero_division=0)\n",
    "\n",
    "# Model B\n",
    "clfb = LogisticRegression(max_iter=200)\n",
    "clfb.fit(XtrainB_scaled, y_trainB)\n",
    "yhatB = clfb.predict(XtestB_scaled)\n",
    "print(\"\\n\\n Model B: Linear Regression Imputation: \")\n",
    "print(classification_report(y_testB, yhatB, digits=3))\n",
    "accB = accuracy_score(y_testB, yhatB)\n",
    "precB, recB, f1B, _ = precision_recall_fscore_support(y_testB, yhatB, average=\"binary\", zero_division=0)\n",
    "\n",
    "# Model C\n",
    "clfc = LogisticRegression(max_iter=200)\n",
    "clfc.fit(XtrainC_scaled, y_trainC)\n",
    "yhatC = clfc.predict(XtestC_scaled)\n",
    "print(\"\\n\\n Model C: Non-Linear Regression Imputation: \")\n",
    "print(classification_report(y_testC, yhatC, digits=3))\n",
    "accC = accuracy_score(y_testC, yhatC)\n",
    "precC, recC, f1C, _ = precision_recall_fscore_support(y_testC, yhatC, average=\"binary\", zero_division=0)\n",
    "\n",
    "# Model D\n",
    "clfd = LogisticRegression(max_iter=200)\n",
    "clfd.fit(XtrainD_scaled, y_trainD)\n",
    "yhatD = clfd.predict(XtestD_scaled)\n",
    "print(\"\\n\\n Model D: Listwise Deletion: \")\n",
    "print(classification_report(y_testD, yhatD, digits=3))\n",
    "accD = accuracy_score(y_testD, yhatD)\n",
    "precD, recD, f1D, _ = precision_recall_fscore_support(y_testD, yhatD, average=\"binary\", zero_division=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c157f2e",
   "metadata": {},
   "source": [
    "## Part C: Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4335f",
   "metadata": {},
   "source": [
    "### Question 1 : Results Comparison \n",
    "\n",
    "Create a summary table comparing the performance metrics (especially F1-score) of the four models:\n",
    "- Model A (Median Imputation)\n",
    "- Model B (Linear Regression Imputation)\n",
    "- Model C (Non-Linear Regression Imputation)\n",
    "- Model D (Listwise Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "210025b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model A: Median Imputation</th>\n",
       "      <td>0.8168</td>\n",
       "      <td>0.6792</td>\n",
       "      <td>0.3255</td>\n",
       "      <td>0.4401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model B: Linear Regression Imputation</th>\n",
       "      <td>0.8168</td>\n",
       "      <td>0.6792</td>\n",
       "      <td>0.3255</td>\n",
       "      <td>0.4401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model C: Non-Linear Regression Imputation</th>\n",
       "      <td>0.8168</td>\n",
       "      <td>0.6792</td>\n",
       "      <td>0.3255</td>\n",
       "      <td>0.4401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model D: Listwise Deletion</th>\n",
       "      <td>0.8196</td>\n",
       "      <td>0.6979</td>\n",
       "      <td>0.3284</td>\n",
       "      <td>0.4467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Accuracy  Precision  Recall      F1\n",
       "Model                                                                         \n",
       "Model A: Median Imputation                   0.8168     0.6792  0.3255  0.4401\n",
       "Model B: Linear Regression Imputation        0.8168     0.6792  0.3255  0.4401\n",
       "Model C: Non-Linear Regression Imputation    0.8168     0.6792  0.3255  0.4401\n",
       "Model D: Listwise Deletion                   0.8196     0.6979  0.3284  0.4467"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary table\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"Model A: Median Imputation\",\n",
    "        \"Model B: Linear Regression Imputation\",\n",
    "        \"Model C: Non-Linear Regression Imputation\",\n",
    "        \"Model D: Listwise Deletion\",\n",
    "    ],\n",
    "    \"Accuracy\": [accA, accB, accC, accD],\n",
    "    \"Precision\": [precA, precB, precC, precD],\n",
    "    \"Recall\": [recA, recB, recC, recD],\n",
    "    \"F1\": [f1A, f1B, f1C, f1D],\n",
    "}).set_index(\"Model\").round(4)\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab7f37d",
   "metadata": {},
   "source": [
    "### Question 2 : Efficacy Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10219c0",
   "metadata": {},
   "source": [
    "#### Question 2.1\n",
    "\n",
    "Discuss the trade-off between **Listwise Deletion (Model D)** and **Imputation (Models A, B, C)**. Why might Model D perform poorly even if the imputed models perform worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4d316",
   "metadata": {},
   "source": [
    "- Listwise Deletion (Model D) removes all rows that contain missing values.  \n",
    "- Imputation (Models A, B, C) keeps every record by estimating and filling missing values.  \n",
    "\n",
    "- Listwise Deletion keeps only complete rows and reduces the dataset size.  \n",
    "- Imputation keeps the full dataset but adds estimated values that may include small errors.  \n",
    "\n",
    "- Deletion assumes data are Missing Completely at Random (MCAR).  \n",
    "- Imputation assumes data are Missing at Random (MAR).  \n",
    "\n",
    "- Deletion may lead to loss of information and bias if the missingness is not random.  \n",
    "- Imputation retains the overall data distribution and preserves statistical power.  \n",
    "\n",
    "- Model D might perform poorly because it trains on a smaller and less diverse subset of the data.  \n",
    "- Important patterns may be lost when rows with missing values are removed.  \n",
    "- The model can become biased because the remaining data may not represent the whole population.  \n",
    "- With fewer samples, the model may overfit and generalize poorly.  \n",
    "- Even if Model D shows slightly higher precision, its recall and robustness can drop because it learns from limited data.  \n",
    "\n",
    "- Listwise Deletion is simpler but risky when missingness is systematic.  \n",
    "- Imputation methods are better for maintaining dataset size and realistic representation.  \n",
    "- In most real-world cases, imputation provides more reliable and generalizable results than deleting data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea442dc",
   "metadata": {},
   "source": [
    "#### Question 2.2\n",
    "\n",
    "Which regression method (Linear vs. Non-Linear) performed better and why? Relate this to the assumed relationship between the imputed feature and the\n",
    "predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efc950",
   "metadata": {},
   "source": [
    "- Both the Linear Regression and Non-Linear Regression methods produced almost the same results.  \n",
    "- The small difference indicates that the imputed feature (AGE) has a mostly linear relationship with other predictors.  \n",
    "- In such cases, the Linear Regression model captures most of the variation effectively.  \n",
    "- Non-linear models like KNN or Decision Tree do not add much benefit when the relationship is already linear.  \n",
    "- Non-linear methods are more useful when the target feature has complex or curved relationships with other variables.  \n",
    "- Since AGE and financial variables (like bill amounts and payment status) have a simple and smooth relationship, Linear Regression imputation was sufficient.  \n",
    "- The small numerical differences between Models B and C confirm that the relationship is approximately linear.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67fd36",
   "metadata": {},
   "source": [
    "#### Question 2.3 \n",
    "\n",
    "Conclude with a recommendation on the best strategy for handling missing data in this scenario, justifying your answer by referencing both the classification performance metrics and the conceptual implications of each method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c43d06",
   "metadata": {},
   "source": [
    "- Listwise Deletion (Model D) achieved slightly higher accuracy but reduced the dataset size.  \n",
    "- The deletion method removes valuable information and can introduce bias if data are not missing completely at random.  \n",
    "- Imputation methods (Models A, B, and C) kept the full dataset and maintained consistent performance across metrics.  \n",
    "- Among the imputation methods, Linear Regression Imputation (Model B) performed nearly as well as the Non-Linear model but with lower complexity.  \n",
    "- This indicates that the missing feature (AGE) has a mostly linear relationship with other predictors.  \n",
    "- The linear approach is both accurate and efficient for this dataset.  \n",
    "- From both performance and conceptual viewpoints, Linear Regression Imputation is the best overall strategy.  \n",
    "- It provides a good balance between data preservation, simplicity, and predictive reliability compared to listwise deletion or complex non-linear imputations.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
